{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDS + FDS\n",
    "from collections import Counter\n",
    "import torch\n",
    "from fds import FDS\n",
    "from scipy.ndimage import convolve1d\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import triang\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "data = pd.read_csv('./data/dataset.csv')\n",
    "categorical = ['Unnamed: 0','track_id','track_genre','explicit', 'mode', 'key', 'time_signature']\n",
    "data = data.drop(categorical, axis=1)\n",
    "# Define the binning function for 'popularity'\n",
    "def get_bin_idx(label, num_bins=10):\n",
    "    bin_width = 100 / num_bins\n",
    "    bin_idx = int(label // bin_width)\n",
    "    return min(bin_idx, num_bins - 1)  # Ensure it does not go out of bounds\n",
    "\n",
    "# Binning the 'popularity' scores\n",
    "bin_index_per_label = [get_bin_idx(label) for label in data['popularity']]\n",
    "\n",
    "# Calculate the number of bins and empirical label distribution\n",
    "Nb = max(bin_index_per_label) + 1\n",
    "num_samples_of_bins = dict(Counter(bin_index_per_label))\n",
    "emp_label_dist = [num_samples_of_bins.get(i, 0) for i in range(Nb)]\n",
    "\n",
    "# Define and get the LDS kernel window\n",
    "def get_lds_kernel_window(kernel='gaussian', ks=5, sigma=2):\n",
    "    assert kernel in ['gaussian', 'triang', 'laplace']\n",
    "    half_ks = (ks - 1) // 2\n",
    "    if kernel == 'gaussian':\n",
    "        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n",
    "        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))\n",
    "    elif kernel == 'triang':\n",
    "        kernel_window = triang(ks)\n",
    "    else:\n",
    "        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n",
    "        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))\n",
    "\n",
    "    return kernel_window\n",
    "\n",
    "lds_kernel_window = get_lds_kernel_window()\n",
    "\n",
    "# Apply the convolution to get the effective label distribution\n",
    "eff_label_dist = convolve1d(np.array(emp_label_dist), weights=lds_kernel_window, mode='constant')\n",
    "\n",
    "eff_label_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 100, num=11)  # Adjust based on how 'popularity' was binned\n",
    "data['bin_index'] = np.digitize(data['popularity'], bins) - 1\n",
    "data['bin_index'] = data['bin_index'].clip(0, 10-1)   # Assign bins  # total number of samples in your dataset\n",
    "total_samples = len(data)  # total number of samples in your dataset\n",
    "weights = 1 / eff_label_dist  # inversely proportional to distribution\n",
    "weights_normalized = weights / weights.sum() * total_samples\n",
    "data['weight'] = data['bin_index'].map(lambda x: weights_normalized[x])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(data.drop(['popularity', 'bin_index', 'weight'], axis=1))\n",
    "y = data['popularity']\n",
    "\n",
    "# Split data, including weights\n",
    "X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(\n",
    "    X_scaled, y, data['weight'], test_size=0.2, random_state=42\n",
    ")\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_numpy = y_test.to_numpy()\n",
    "y_test_torch = torch.tensor(y_test_numpy, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds = FDS(feature_dim=X_train_torch.shape[1],start_smooth=2)\n",
    "X_train_smoothed_torch = fds.smooth(X_train_torch, y_train_torch, epoch=1)\n",
    "X_train_smoothed = X_train_smoothed_torch.numpy()\n",
    "\n",
    "regressorDIR = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
    "regressorDIR.fit(X_train_smoothed, y_train, sample_weight=weights_train)\n",
    "X_test_smoothed_torch = fds.smooth(X_test_torch, y_test_numpy, epoch=1)  # Same epoch as above\n",
    "X_test_smoothed = X_test_smoothed_torch.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressorDIR.predict(X_test_smoothed)\n",
    "mse = mean_squared_error(y_test, y_pred, sample_weights = weights_test)\n",
    "rmse = mse ** 0.5\n",
    "r2 = r2_score(y_test, y_pred, sample_weights = weights_test)\n",
    "mse, rmse, r2"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
