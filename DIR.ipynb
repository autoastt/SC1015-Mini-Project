{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "from utils.fds import FDS\n",
    "from scipy.ndimage import convolve1d\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import triang\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/data_smogn_05.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22111, 28429, 35966, 32890, 40154, 58912, 57908, 63167, 56074,\n",
       "       43001])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the binning function for 'popularity'\n",
    "def get_bin_idx(label, num_bins=10):\n",
    "    bin_width = 100 / num_bins\n",
    "    bin_idx = int(label // bin_width)\n",
    "    return min(bin_idx, num_bins - 1)  # Ensure it does not go out of bounds\n",
    "\n",
    "# Binning the 'popularity' scores\n",
    "bin_index_per_label = [get_bin_idx(label) for label in data['popularity']]\n",
    "\n",
    "# Calculate the number of bins and empirical label distribution\n",
    "Nb = max(bin_index_per_label) + 1\n",
    "num_samples_of_bins = dict(Counter(bin_index_per_label))\n",
    "emp_label_dist = [num_samples_of_bins.get(i, 0) for i in range(Nb)]\n",
    "\n",
    "# Define and get the LDS kernel window\n",
    "def get_lds_kernel_window(kernel='gaussian', ks=5, sigma=2):\n",
    "    assert kernel in ['gaussian', 'triang', 'laplace']\n",
    "    half_ks = (ks - 1) // 2\n",
    "    if kernel == 'gaussian':\n",
    "        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n",
    "        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))\n",
    "    elif kernel == 'triang':\n",
    "        kernel_window = triang(ks)\n",
    "    else:\n",
    "        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n",
    "        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))\n",
    "\n",
    "    return kernel_window\n",
    "\n",
    "lds_kernel_window = get_lds_kernel_window()\n",
    "\n",
    "# Apply the convolution to get the effective label distribution\n",
    "eff_label_dist = convolve1d(np.array(emp_label_dist), weights=lds_kernel_window, mode='constant')\n",
    "\n",
    "eff_label_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>bin_index</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67.0</td>\n",
       "      <td>351413.0</td>\n",
       "      <td>0.437119</td>\n",
       "      <td>0.496777</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038004</td>\n",
       "      <td>0.621056</td>\n",
       "      <td>0.055218</td>\n",
       "      <td>0.077442</td>\n",
       "      <td>0.329562</td>\n",
       "      <td>127.922188</td>\n",
       "      <td>6</td>\n",
       "      <td>7212.505044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68.0</td>\n",
       "      <td>351783.0</td>\n",
       "      <td>0.427469</td>\n",
       "      <td>0.615662</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.028970</td>\n",
       "      <td>0.348779</td>\n",
       "      <td>0.178368</td>\n",
       "      <td>0.102032</td>\n",
       "      <td>0.201814</td>\n",
       "      <td>92.113026</td>\n",
       "      <td>6</td>\n",
       "      <td>7212.505044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.0</td>\n",
       "      <td>351661.0</td>\n",
       "      <td>0.509178</td>\n",
       "      <td>0.543150</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.027034</td>\n",
       "      <td>0.234267</td>\n",
       "      <td>0.110648</td>\n",
       "      <td>0.084248</td>\n",
       "      <td>0.328488</td>\n",
       "      <td>114.429474</td>\n",
       "      <td>6</td>\n",
       "      <td>7212.505044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67.0</td>\n",
       "      <td>351394.0</td>\n",
       "      <td>0.437674</td>\n",
       "      <td>0.490773</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038458</td>\n",
       "      <td>0.635044</td>\n",
       "      <td>0.048935</td>\n",
       "      <td>0.076265</td>\n",
       "      <td>0.335983</td>\n",
       "      <td>129.716280</td>\n",
       "      <td>6</td>\n",
       "      <td>7212.505044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68.0</td>\n",
       "      <td>351567.0</td>\n",
       "      <td>0.572096</td>\n",
       "      <td>0.487288</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025623</td>\n",
       "      <td>0.153533</td>\n",
       "      <td>0.057433</td>\n",
       "      <td>0.071576</td>\n",
       "      <td>0.424923</td>\n",
       "      <td>131.394058</td>\n",
       "      <td>6</td>\n",
       "      <td>7212.505044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106364</th>\n",
       "      <td>40.0</td>\n",
       "      <td>246306.0</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>-4.722</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>128.002000</td>\n",
       "      <td>4</td>\n",
       "      <td>10401.497785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106365</th>\n",
       "      <td>38.0</td>\n",
       "      <td>312566.0</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>-4.722</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.246000</td>\n",
       "      <td>0.427000</td>\n",
       "      <td>113.949000</td>\n",
       "      <td>3</td>\n",
       "      <td>12698.745578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106366</th>\n",
       "      <td>21.0</td>\n",
       "      <td>384999.0</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>-16.393</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.086300</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>125.995000</td>\n",
       "      <td>2</td>\n",
       "      <td>11612.682591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106367</th>\n",
       "      <td>41.0</td>\n",
       "      <td>283893.0</td>\n",
       "      <td>0.587000</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>-10.889</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>135.960000</td>\n",
       "      <td>4</td>\n",
       "      <td>10401.497785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106368</th>\n",
       "      <td>22.0</td>\n",
       "      <td>241826.0</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>0.487000</td>\n",
       "      <td>-10.204</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.681000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089300</td>\n",
       "      <td>0.708000</td>\n",
       "      <td>79.198000</td>\n",
       "      <td>2</td>\n",
       "      <td>11612.682591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106369 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        popularity  duration_ms  danceability    energy  loudness  \\\n",
       "0             67.0     351413.0      0.437119  0.496777     0.000   \n",
       "1             68.0     351783.0      0.427469  0.615662     0.000   \n",
       "2             68.0     351661.0      0.509178  0.543150     0.000   \n",
       "3             67.0     351394.0      0.437674  0.490773     0.000   \n",
       "4             68.0     351567.0      0.572096  0.487288     0.000   \n",
       "...            ...          ...           ...       ...       ...   \n",
       "106364        40.0     246306.0      0.470000  0.938000    -4.722   \n",
       "106365        38.0     312566.0      0.475000  0.860000    -4.722   \n",
       "106366        21.0     384999.0      0.172000  0.235000   -16.393   \n",
       "106367        41.0     283893.0      0.587000  0.506000   -10.889   \n",
       "106368        22.0     241826.0      0.526000  0.487000   -10.204   \n",
       "\n",
       "        speechiness  acousticness  instrumentalness  liveness   valence  \\\n",
       "0          0.038004      0.621056          0.055218  0.077442  0.329562   \n",
       "1          0.028970      0.348779          0.178368  0.102032  0.201814   \n",
       "2          0.027034      0.234267          0.110648  0.084248  0.328488   \n",
       "3          0.038458      0.635044          0.048935  0.076265  0.335983   \n",
       "4          0.025623      0.153533          0.057433  0.071576  0.424923   \n",
       "...             ...           ...               ...       ...       ...   \n",
       "106364     0.105000      0.000529          0.000000  0.251000  0.453000   \n",
       "106365     0.042100      0.006500          0.000002  0.246000  0.427000   \n",
       "106366     0.042200      0.640000          0.928000  0.086300  0.033900   \n",
       "106367     0.029700      0.381000          0.000000  0.270000  0.413000   \n",
       "106368     0.072500      0.681000          0.000000  0.089300  0.708000   \n",
       "\n",
       "             tempo  bin_index        weight  \n",
       "0       127.922188          6   7212.505044  \n",
       "1        92.113026          6   7212.505044  \n",
       "2       114.429474          6   7212.505044  \n",
       "3       129.716280          6   7212.505044  \n",
       "4       131.394058          6   7212.505044  \n",
       "...            ...        ...           ...  \n",
       "106364  128.002000          4  10401.497785  \n",
       "106365  113.949000          3  12698.745578  \n",
       "106366  125.995000          2  11612.682591  \n",
       "106367  135.960000          4  10401.497785  \n",
       "106368   79.198000          2  11612.682591  \n",
       "\n",
       "[106369 rows x 13 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = np.linspace(0, 100, num=11)  # Adjust based on how 'popularity' was binned\n",
    "data['bin_index'] = np.digitize(data['popularity'], bins) - 1\n",
    "data['bin_index'] = data['bin_index'].clip(0, 10-1)   # Assign bins  # total number of samples in your dataset\n",
    "total_samples = len(data)  # total number of samples in your dataset\n",
    "weights = 1 / eff_label_dist  # inversely proportional to distribution\n",
    "weights_normalized = weights / weights.sum() * total_samples\n",
    "data['weight'] = data['bin_index'].map(lambda x: weights_normalized[x])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Normalize features\n",
    "scaler = Normalizer()\n",
    "X_scaled = scaler.fit_transform(data.drop(['popularity', 'bin_index', 'weight'], axis=1))\n",
    "y = data['popularity']\n",
    "\n",
    "# Split data, including weights\n",
    "X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(\n",
    "    X_scaled, y, data['weight'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fds \u001b[38;5;241m=\u001b[39m \u001b[43mFDS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train_torch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstart_smooth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m X_train_smoothed_torch \u001b[38;5;241m=\u001b[39m fds\u001b[38;5;241m.\u001b[39msmooth(X_train_torch, y_train_torch, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m X_train_smoothed \u001b[38;5;241m=\u001b[39m X_train_smoothed_torch\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/Desktop/SC1015/utils/fds.py:31\u001b[0m, in \u001b[0;36mFDS.__init__\u001b[0;34m(self, feature_dim, bucket_num, bucket_start, start_update, start_smooth, kernel, ks, sigma, momentum)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbucket_num \u001b[38;5;241m=\u001b[39m bucket_num\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbucket_start \u001b[38;5;241m=\u001b[39m bucket_start\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_window \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_kernel_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhalf_ks \u001b[38;5;241m=\u001b[39m (ks \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;241m=\u001b[39m momentum\n",
      "File \u001b[0;32m~/Desktop/SC1015/utils/fds.py:61\u001b[0m, in \u001b[0;36mFDS._get_kernel_window\u001b[0;34m(kernel, ks, sigma)\u001b[0m\n\u001b[1;32m     58\u001b[0m     kernel_window \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(laplace, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m-\u001b[39mhalf_ks, half_ks \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mmap\u001b[39m(laplace, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m-\u001b[39mhalf_ks, half_ks \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing FDS: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msigma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.1.5/libexec/lib/python3.12/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_numpy = y_test.to_numpy()\n",
    "y_test_torch = torch.tensor(y_test_numpy, dtype=torch.float32)\n",
    "\n",
    "fds = FDS(feature_dim = X_train_torch.shape[1],start_smooth=2)\n",
    "X_train_smoothed_torch = fds.smooth(X_train_torch, y_train_torch, epoch=1)\n",
    "X_train_smoothed = X_train_smoothed_torch.numpy()\n",
    "\n",
    "regressorDIR = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
    "regressorDIR.fit(X_train_smoothed, y_train, sample_weight=weights_train)\n",
    "X_test_smoothed_torch = fds.smooth(X_test_torch, y_test_numpy, epoch=1)  # Same epoch as above\n",
    "X_test_smoothed = X_test_smoothed_torch.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressorDIR.predict(X_test_smoothed)\n",
    "mse = mean_squared_error(y_test, y_pred, sample_weights = weights_test)\n",
    "rmse = mse ** 0.5\n",
    "r2 = r2_score(y_test, y_pred, sample_weights = weights_test)\n",
    "mse, rmse, r2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
