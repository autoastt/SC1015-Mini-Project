{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c2f7d94-e8e6-4f25-8c07-a672481a5003",
   "metadata": {},
   "source": [
    "# Models Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82e59bc-84b1-4f97-a5e0-64e906529847",
   "metadata": {},
   "source": [
    "- In this notebook, we will train various models and measure their performance by $MAE$, $MSE$, and $R^2$.\n",
    "- As the purpose of this project is to predict `popularity`, we decided that regression model is the the most appropriate. \n",
    "- Our selected models are the following:\n",
    "    1. Decision Tree\n",
    "    2. AdaBoost\n",
    "    3. Random Forest\n",
    "    4. Gradient Boosting (scikit-learn)\n",
    "    5. Hist Gradient Boosting (scikit-learn)\n",
    "    6. XGBoost\n",
    "    7. LightGBM \n",
    "    8. CatBoost\n",
    "    9. K-Nearest Neighbors (KNN)\n",
    "    10. Multilayer Perceptron (MLP)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded00de2-3bb7-4db8-bbdd-c0f2287df742",
   "metadata": {},
   "source": [
    "## Preparing Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de9bb24-f595-478d-a298-59d53605c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e60bb9f7-740a-40ca-8ff3-1b10db76e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def split_data(df, LDS=False, S=False, N=False):\n",
    "    # Define the features and target variable\n",
    "    # X = df.drop(['popularity', 'weight'], axis=1)\n",
    "    X = df.drop(['popularity'], axis=1)\n",
    "    y = df['popularity']\n",
    "    # Splitting the dataset into training and testing sets\n",
    "    if LDS == False: \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        # Split data, including weights\n",
    "        X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(\n",
    "            X, y, df['weight'], test_size=0.2, random_state=42\n",
    "        )\n",
    "        normalizer = Normalizer()\n",
    "        normalizer.fit(X_train)\n",
    "        X_train = normalizer.transform(X_train)\n",
    "        X_test = normalizer.transform(X_test)\n",
    "        return X_train, X_test, y_train, y_test, weights_train, weights_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "97cf8282-d683-493e-9e94-7b4f1eda066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def model_performace(model_name, y_test, y_pred, weights_test=None):\n",
    "    mae = mean_absolute_error(y_test, y_pred, sample_weight=weights_test)\n",
    "    mse = mean_squared_error(y_test, y_pred, sample_weight=weights_test)\n",
    "    r2 = r2_score(y_test, y_pred, sample_weight=weights_test)\n",
    "    print(f\"Model Performance ({model_name}):\\nMAE = {mae}\\nMSE = {mse}\\nR^2 = {r2}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c535ca10-3b0c-44cf-b053-a86f88ae1ece",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f01df0-559d-46c2-9517-4e8e6581cc60",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c309d3b8-2064-46b1-96e9-cbbb4825e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "750f3eb8-d4ee-45f1-b3f3-d285f8aac964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Models:\n",
    "    def __init__(self, df, LDS=False):\n",
    "        self.df = df\n",
    "        self.LDS = LDS\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = split_data(df)\n",
    "        \n",
    "        scaler = StandardScaler()  \n",
    "        scaler.fit(self.X_train)\n",
    "        self.X_train_standardized = scaler.transform(self.X_train)  \n",
    "        self.X_test_standardized = scaler.transform(self.X_test)  \n",
    "\n",
    "        normalizer = Normalizer()\n",
    "        normalizer.fit(self.X_train)\n",
    "        self.X_train_normalized = normalizer.transform(self.X_train)\n",
    "        self.X_test_normalized = normalizer.transform(self.X_test)\n",
    "        \n",
    "        if self.LDS == True: \n",
    "            self.X_train_LDS, self.X_test_LDS, self.y_train_LDS, self.y_test_LDS, self.w_train, self.w_test = split_data(df, LDS=True)\n",
    "\n",
    "        self.DecisionTree()\n",
    "        self.AdaBoost()\n",
    "        self.RandomForest()\n",
    "        self.GB()\n",
    "        self.HistGB()\n",
    "        self.XGBoost()\n",
    "        self.LGBM()\n",
    "        self.CatBoost()\n",
    "        self.KNN()\n",
    "        self.MLP()\n",
    "\n",
    "    def run_model(self, regr, model_name):\n",
    "        regr.fit(self.X_train, self.y_train)\n",
    "        y_pred = regr.predict(self.X_test)\n",
    "        model_performace(model_name=f\"{model_name}\", y_test=self.y_test, y_pred=y_pred)\n",
    "        if self.LDS == False: \n",
    "            return\n",
    "        regr.fit(self.X_train_LDS, self.y_train_LDS, sample_weight=self.w_train)\n",
    "        y_pred = regr.predict(self.X_test_LDS)\n",
    "        model_performance(model_name=f\"{model_name} (with LDS)\", y_test=self.y_test_LDS, y_pred=y_pred, weights_test=self.w_test)\n",
    "    \n",
    "    def DecisionTree(self):\n",
    "        regr = DecisionTreeRegressor()\n",
    "        self.run_model(regr, \"Decision Tree\")\n",
    "    \n",
    "    def AdaBoost(self):\n",
    "        regr = AdaBoostRegressor(DecisionTreeRegressor(), n_estimators=50)\n",
    "        self.run_model(regr, \"AdaBoost\")\n",
    "\n",
    "    def RandomForest(self):\n",
    "        regr = RandomForestRegressor(n_estimators=100)\n",
    "        self.run_model(regr, \"Random Forest\")\n",
    "\n",
    "    def GB(self):\n",
    "        regr = GradientBoostingRegressor(n_estimators=100)\n",
    "        self.run_model(regr, \"Gradient Boosting\")\n",
    "\n",
    "    def HistGB(self):\n",
    "        regr = HistGradientBoostingRegressor(max_iter=100)\n",
    "        self.run_model(regr, \"Hist Gradient Boosting\")\n",
    "\n",
    "    def XGBoost(self):\n",
    "        regr = XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
    "        self.run_model(regr, \"XGBoost\")\n",
    "\n",
    "    def LGBM(self):\n",
    "        regr = lgb.LGBMRegressor()\n",
    "        regr.fit(self.X_train, self.y_train, eval_set=[(self.X_test, self.y_test)], eval_metric='mse')\n",
    "        y_pred = regr.predict(self.X_test, num_iteration=regr.best_iteration_)\n",
    "        model_performace(model_name=f\"LightGBM\", y_test=self.y_test, y_pred=y_pred)\n",
    "\n",
    "    def CatBoost(self):\n",
    "        regr = CatBoostRegressor(verbose=0)\n",
    "        regr.fit(self.X_train, self.y_train, eval_set=(self.X_test, self.y_test), use_best_model=True)\n",
    "        y_pred = regr.predict(self.X_test)\n",
    "        model_performace(model_name=f\"CatBoost\", y_test=self.y_test, y_pred=y_pred)\n",
    "\n",
    "    def KNN(self):\n",
    "        regr = KNeighborsRegressor(n_neighbors=5, weights='distance')\n",
    "        regr.fit(self.X_train_normalized, self.y_train)\n",
    "        y_pred = regr.predict(self.X_test_normalized)\n",
    "        model_performace(model_name=f\"K-Nearest Neighbors\", y_test=self.y_test, y_pred=y_pred)\n",
    "\n",
    "    def MLP(self):\n",
    "        params = { 'hidden_layer_sizes' : [10,10],\n",
    "            'activation' : 'relu', 'solver' : 'adam',\n",
    "            'alpha' : 0.0, 'batch_size' : 10,\n",
    "            'random_state' : 0, 'tol' : 0.0001,\n",
    "            'nesterovs_momentum' : False,\n",
    "            'learning_rate' : 'constant',\n",
    "            'learning_rate_init' : 0.01,\n",
    "            'max_iter' : 1000, 'shuffle' : True,\n",
    "            'n_iter_no_change' : 50, 'verbose' : False }\n",
    "        regr = MLPRegressor(**params)\n",
    "        regr.fit(self.X_train_standardized, self.y_train)\n",
    "        y_pred = regr.predict(self.X_test_standardized)\n",
    "        model_performace(model_name=f\"Multilayer Perceptron\", y_test=self.y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b97df8ad-b768-40ed-bef5-b3d43c0ac7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000421 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 85095, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 58.119126\n",
      "Model Performance (LightGBM):\n",
      "MAE = 15.355236282429157\n",
      "MSE = 366.60797458236794\n",
      "R^2 = 0.6442629189061209\n",
      "\n",
      "Model Performance (CatBoost):\n",
      "MAE = 14.165357546553388\n",
      "MSE = 325.87661093465897\n",
      "R^2 = 0.6837864901800835\n",
      "\n",
      "Model Performance (K-Nearest Neighbors):\n",
      "MAE = 11.076730730513027\n",
      "MSE = 306.73285513191905\n",
      "R^2 = 0.7023625831256848\n",
      "\n",
      "Model Performance (Multilayer Perceptron):\n",
      "MAE = 16.253345010523233\n",
      "MSE = 422.04907163627735\n",
      "R^2 = 0.590465796622931\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_df = pd.read_csv(\"./data/data_smogn_05.csv\")\n",
    "model = Models(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a104a39b-c56d-4d89-9233-a2e76292574d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
